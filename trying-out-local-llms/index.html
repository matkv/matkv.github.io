<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Trying out local LLMs | matkv.dev</title><link rel=stylesheet href=/css/main.css integrity crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/iconoir-icons/iconoir@main/css/iconoir.css><link disabled id=dark-css rel=stylesheet href=/css/dark.css><link id=light-syntax rel=stylesheet href=/css/syntax-light.css><link id=dark-syntax rel=stylesheet href=/css/syntax-dark.css disabled><link rel=apple-touch-icon sizes=57x57 href=/icons/apple-touch-icon-57x57.png><link rel=apple-touch-icon sizes=60x60 href=/icons/apple-touch-icon-60x60.png><link rel=apple-touch-icon sizes=72x72 href=/icons/apple-touch-icon-72x72.png><link rel=apple-touch-icon sizes=76x76 href=/icons/apple-touch-icon-76x76.png><link rel=apple-touch-icon sizes=114x114 href=/icons/apple-touch-icon-114x114.png><link rel=apple-touch-icon sizes=120x120 href=/icons/apple-touch-icon-120x120.png><link rel=apple-touch-icon sizes=144x144 href=/icons/apple-touch-icon-144x144.png><link rel=apple-touch-icon sizes=152x152 href=/icons/apple-touch-icon-152x152.png><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon-180x180.png><link rel=icon type=image/png sizes=192x192 href=/icons/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=/icons/favicon-32x32.png><link rel=icon type=image/png sizes=96x96 href=/icons/favicon-96x96.png><link rel=icon type=image/png sizes=16x16 href=/icons/favicon-16x16.png><link rel=manifest href=/icons/manifest.json><meta name=msapplication-TileColor content="#ffffff"><meta name=msapplication-TileImage content="/icons/mstile-144x144.png"><meta name=theme-color content="#ffffff"><script src=/js/lightbox.js></script><script src=/js/main.min.a9421d68ec998e844eae917085f8b86d1b9dedc69734e7cd1f8aff56cc0d1cff.js integrity="sha256-qUIdaOyZjoROrpFwhfi4bRud7caXNOfNH4r/VswNHP8=" crossorigin=anonymous></script></head><body><header><nav><ul><li><a href=/>Home</a></li><li><a href=/about>About</a></li><li><a href=/log>Log</a></li><li><a href=/tags>Tags</a></li><li><a href=/archive>Archive</a></li><div class=nav-icons><a href=/search><li><i id=searchicon class=iconoir-search></i></li></a><li><i id=darkmodeswitch class=iconoir-half-moon></i>
<script>var savedTheme,toggle=document.getElementById("darkmodeswitch"),darkTheme=document.getElementById("dark-css"),lightSyntax=document.getElementById("light-syntax"),darkSyntax=document.getElementById("dark-syntax");toggle.addEventListener("click",()=>{toggle.className==="iconoir-half-moon"?setTheme("dark"):toggle.className==="iconoir-sun-light"&&setTheme("light")}),savedTheme=localStorage.getItem("dark-mode-storage")||"light",setTheme(savedTheme);function setTheme(e){localStorage.setItem("dark-mode-storage",e),e==="dark"?(darkTheme.disabled=!1,darkSyntax.disabled=!1,lightSyntax.disabled=!0,toggle.className="iconoir-sun-light"):e==="light"&&(darkTheme.disabled=!0,darkSyntax.disabled=!0,lightSyntax.disabled=!1,toggle.className="iconoir-half-moon")}</script></li></div></ul></nav><hr></header><main><h1>Trying out local LLMs</h1><div class=date><time datetime=2025-06-12>12 June, 2025</time></div><div class=tag-container><span class=tag-label>Tags:</span><div class=tags><ul><li><a href=/tags/ai/>AI</a></li><li><a href=/tags/llms/>LLMs</a></li><li><a href=/tags/tools/>Tools</a></li></ul></div></div><p>I&rsquo;ve been trying out local LLMs for the first time this weekend. I installed <a href=https://ollama.com/>Ollama</a> locally on Windows and then downloaded two models just to play around with them: Deepseek R1 & LLama 2 Uncensored.</p><p>I wrote a little <a href=https://wiki.matkv.dev/local-llms>tutorial</a> for myself that lets me download Ollama & the models to a different drive than the default location because they take up quite a bit of storage space. I also think it&rsquo;s wild that you basically need as much free <strong>RAM</strong> as the <strong>size of the model</strong> in order to run it. So for the models with huge amounts of parameters you need some pretty insane setups.</p><p>Overall I think it&rsquo;s pretty cool that you can run an LLM completely locally - basically without any restrictions on how often you can ask a question (in comparison to chatGPT for example), and also with faster response times - the LLama 2 model responds pretty much instantaneously for me. The Deepseek model seems to show it&rsquo;s thinking process in greyed out text for quite a while before actually answering. So this could be a cool way to still have a local copilot even when I don&rsquo;t have internet access. Fun to mess around with it in any case.</p><p>I read that these local models are supposed to be a bit less restricted & uncensored in comparison to the online models but from playing around with it that hasn&rsquo;t really been the case for me. Maybe I need to try other models for that.</p><p>Maybe I&rsquo;ll have an idea for a usecase for a local LLM in a software project, can&rsquo;t think of anything specific yet though.</p></main><footer><div class=footer><div class=construction>This site is still a work in progress, some content might be missing or incomplete & the styling might look
weird.</div><p>&copy; 2025 | matkv</p></div></footer></body></html>