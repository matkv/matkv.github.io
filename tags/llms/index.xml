<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLMs on matkv.dev</title><link>https://matkv.dev/tags/llms/</link><description>Recent content in LLMs on matkv.dev</description><generator>Hugo</generator><language>en-us</language><copyright>test</copyright><lastBuildDate>Thu, 12 Jun 2025 18:17:48 +0200</lastBuildDate><atom:link href="https://matkv.dev/tags/llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Trying out local LLMs</title><link>https://matkv.dev/trying-out-local-llms/</link><pubDate>Thu, 12 Jun 2025 18:17:48 +0200</pubDate><guid>https://matkv.dev/trying-out-local-llms/</guid><description>&lt;p&gt;I&amp;rsquo;ve been trying out local LLMs for the first time this weekend. I installed &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; locally on Windows and then downloaded two models just to play around with them: Deepseek R1 &amp;amp; LLama 2 Uncensored.&lt;/p&gt;
&lt;p&gt;I wrote a little &lt;a href="https://wiki.matkv.dev/local-llms"&gt;tutorial&lt;/a&gt; for myself that lets me download Ollama &amp;amp; the models to a different drive than the default location because they take up quite a bit of storage space. I also think it&amp;rsquo;s wild that you basically need as much free &lt;strong&gt;RAM&lt;/strong&gt; as the &lt;strong&gt;size of the model&lt;/strong&gt; in order to run it. So for the models with huge amounts of parameters you need some pretty insane setups.&lt;/p&gt;</description></item></channel></rss>